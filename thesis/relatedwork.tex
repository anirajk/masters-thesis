%%% -*-LaTeX-*-

\chapter{Related Work}
\label{chap:relatedwork}

In-memory databases and low latency networks are very heavily researched areas
these days. Our work borrows concepts from numerous efforts both current and 
prior from the databases, networking, and operating systems research communities.
RAMCloud~\cite{ramcloud} itself was a research product built on top of other 
work on log-structured file systems and kernel by-pass networking. RAMCloud gave the 
world a distributed fast crash recovery, scalable log structured DRAM storage, and a new consensus 
algorithm~\cite{ryan-thesis,ongaro2011fast,ramcloudfast,raft} in addition to many other 
functionalities that makes it one of the most feature complete low latency storage systems 
to come out of research.

\section{In-memory Databases}
In-memory databases sparked a new wave of systems research in the 
networking and database communities. For a long time while developing storage
systems, researchers did not diverge from the  tried and tested model for the 
locality of reference. Conventional ``secondary storage" devices
such as disks were the final resting place of ever-growing amounts of managed data. With the 
enhanced requirements of scale by the popularization of Web 2.0 in the early 2000s, 
database designers created a caching layer of RAM in front of disks
to combat abysmal disk access times that were dragging down performance at scale. Memcached~\cite{memcached-orig}
is one such popular system and its further development
enabled it to operate at a scale of billions of operations per second on trillions of items~\cite{nishtala2013scaling}.

Another orthogonal area of research was to completely store all data in main memory.
This was made possible in the late 2000s by the evolution of DRAM technologies and declining cost of main memory.
Questions about durability and cost of replication 
were responded to by means of distributed recovery~\cite{ongaro2011fast} and advances in defragmentation techniques~\cite{ramcloudfast} 
ensured high DRAM utilization. Another recent development was in the acceptance of high throughput and low latency networks. 
Pioneered by the high performance computing community, these advanced networking fabrics 
such as Infiniband~\cite{pfister2001introduction} became more commonplace in 
data center networks, and kernel by-pass networking was made ubiquitous by packet processing 
frameworks such as DPDK~\cite{dpdk}, which enabled them to operate over universally accepted protocols
such as Ethernet. These developments have led to a deluge of storage systems research~\cite{mmdbmstutorial} 
exploring multiple verticals. Among the particularly notable ones are RAMCloud~\cite{ramcloud},
FaRM~\cite{farm}, and H-Store~\cite{hstore} (scale-out memory stores) and
MICA~\cite{mica}, HyPer~\cite{hyper}, and HERD~\cite{herd} (focused on single node performance).
While each of these systems has been driven by different dimensions 
of research, they all share the same concerns of in-memory storage systems 
operating on high-speed networks.

Today's main memory database systems offer a plethora of features that are at par with traditional
database systems including structured storage and indexing, concurrency control and transactions,
 durability and recovery techniques, query processing and compilation, support for high availability, and 
ability to support hybrid transactional and analytics workloads~\cite{mmdbmstutorial}.
FaRM~\cite{farm} is an in-memory, distributed computing platform that leverages lock-free reads over RDMA~\cite{rdma},
collocating objects and function shipping to provide performance characteristics of the 
highest order for an in-memory database. FaRM scales distributed transactions to
hundreds of millions of operations with recovery times of the order of milliseconds by relying on 
full replication in DRAM. There are other systems such as Silo~\cite{silo} that effectively utilize multicore CPUs.

\subsection{Column Stores}
Column-oriented databases have generated interest in recent years owing to the growing 
need of running complex analytical queries over a large amount of data. Most enterprise 
database solutions today ship with the column stores. This new focus has resulted 
in advanced materialization strategies and compression schemes~\cite{cstore,cstorevsrowstore,cstore-material,cstorecompression}.
In-memory databases with hybrid layouts~\cite{hybridinmemorycolstore} and other recent work on in-memory column stores~\cite{inmemorycracking} 
focus a lot on analytical workloads and techniques like database cracking~\cite{databasecracking} for adaptive indexing. 
MonetDB~\cite{monetdb} made significant milestones in OLAP processing of in-memory column stores. HyPer~\cite{hyperhybrid} 
is an example of a system that underlines the importance of RDMA in the context of a column store.


\subsection{Lock Freedom}
Lock freedom is important for Zero Copy and RDMA since the NIC also introduce contention 
since records are transmitted directly from their original memory location without an 
additional copy. Lock and latch freedom are 
design philosophies perfected on previous work on fine-grained concurrency 
primitives~\cite{finegrained} and lock-free hash tables ~\cite{lockfreeht}. Lock freedom is commonly
used in databases these days~\cite{htm} since they gained practicality by the common use of
epochs-based reclamation~\cite{lockfreedom} . 
More recent iterations of SQL engines such as Hekaton~\cite{hekaton} are 
examples for in-memory optimized  database engines. Bw-Tree~\cite{bw-tree}, a 
data structure that is lock and latch free, follows a no-update-in-place philosophy to indexing.
Others have worked on exploiting Hardware Transactional Memory~\cite{htm-old}
for in-memory transaction processing~\cite{drtm} as well as lock-free indexing~\cite{htm}.

\section{RDMA in Databases}
The merits of RDMA have been widely accepted by database researchers by now. The 
ability to offload CPU from transmission make it appealing while the necessity for 
multiple round trips to fetch data made people hesitant. The ability of Infiniband NICs to 
use scatter gather to assist remote memory access has also been discussed before~\cite{zerocopy04}.
From the early arguments~\cite{rdmacase} calling for the need for offloading CPUs to 
research aiming for a billion key value operations on key value servers ~\cite{rdmabillion},
RDMA seems to be a promising paradigm for the future of large-scale databases. Others have suggested
a decoupled server I/O architecture using Programmable I/O (PIO) and re-architecting virtual 
memory systems for network transfers~\cite{hicamp}. Key value stores
such as HERD~\cite{herd} and MICA~\cite{mica} and systems that provide more complex
data models such as RAMCloud~\cite{ramcloud} use RDMA effectively and there have been 
published guidelines to maximize performance for dispatch-heavy workloads~\cite{rdma}.

\section{Data Migration}
Research efforts focussing on reconfiguration for in-memory databases have seen a lot of 
interest lately. Albatross~\cite{albatross} offered live reconfiguration with unavailability windows as low as 
300ms. Zephyr~\cite{zephyr} focuses on mitigating violations to Service Level Agreements (SLAs) and limits 
change in average latency to 10-20\% but like Albatross, operates on latencies of the order of tens of milliseconds.  
Squall~\cite{squall} employs reactive pulls and pre-partitioning
of data for fast migrations and is the state-of-the-art for low impact migrations that specifically work well for 
heavily skewed workloads. Like others, Squall also operates on platforms with throughput and latencies orders of 
magnitude less than what is possible with today's hardware. The key challenge in implementing migration in a system 
like RAMCloud is to maintain the latencies that are 1000$\times$ tighter than these systems.

\section{Data Transfer in Fast Networks}
As pointed out by various researchers in the past, the days of slow networks are over~\cite{slow} 
and network no longer remains the primary bottleneck that database designers should take for granted.
This thesis argues heavily in favor of that fact. Several efforts in the past have exploited
user-level NIC access as well as RDMA for highly performant distributed database systems~\cite{ramcloud,farm,farmtx,drtm,hyper}
as well as block I/O for file systems~\cite{rdmagfs}. The closest analysis that we have found on 
range queries and in-memory databases~\cite{zerocopyrangequery} argues for use of RDMA reads and 
multisocket parallelisation of copying of data. Our analysis is more detailed and quantifies the 
results with memory bandwidth and effects of DDIO.
Our findings should also complement other efforts such as transactional key value stores~\cite{deuteronomy} 
and shared-data databases~\cite{tell}. Small, fixed-size request/response cycles have been optimized in
existing research~\cite{farm,herd,mica,rdma,ramcloud}, but the efficient
transmission of larger responses like range query results or data
migrations has been less well studied. Studies focused on large transmissions
have so far been limited to relatively static block-oriented data.
Our work focuses on optimizing transmission of large and complex query
results, which differs from these two categories. 

A complementary study by Kalia, Kaminsky, and Andersen~\cite{rdma} provides a
different analysis of host interaction with Mellanox Infiniband network adapters,
and they extract rules of thumb to help developers get the best performance from the hardware.
The low-level nature of their analysis is especially suited for
optimizing dispatch-heavy workloads with a high volume of small
request-response operations. While most of RDMA research prefers one-sided RDMA verbs, more recent efforts have shown 
the benefits of two-sided RDMA and datagram RPCs~\cite{fasst}. We believe that 
two-sided RDMA and RPC semantics are the most promising in today's architecture. In the future, 
richer semantics that allows the gathering of remote data might prove beneficial for one-sided RDMA verbs.


\section{Summary}
Our work is built on the premises of a lot of research ideas and its unique take on
heavy transfers in the presence of tight SLAs (latencies of the order of 10s of microseconds)
make it stand out among the related work. Until now, mixing workloads such as range scans 
and data migration, especially in the context of modern networks, has not been well
 understood from the server side. We present an in-depth analysis of resource utilization, 
 the impact of DDIO and how data layout affects performance and efficiency in a modern 
 in-memory database.
