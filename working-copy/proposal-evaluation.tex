%%% -*-LaTeX-*-

%\chapter{Evaluation}

We explored hows the different designs trade-off database server efficiency and
performance, by building a simple model of an in-memory database system that
concentrates on data transfer rather than full query processing. In all experiments,
one node acts as a server and transmits results to 15 client nodes.
Our experiments were run on the Apt~\cite{Ricci+:OSR15} cluster of the
CloudLab~\cite{Cloudlab:URL} testbed: this testbed provides exclusive bare-metal
access to a large number of machines with RDMA-capable Infiniband NICs.

The proposed work that is pending will give more conclusive evidence for the 


\input{config}
Table~\ref{tbl:config} shows the Experiment Setup for the cluster where we profiled
the Mellanox Infiniband ConnectX-3 \textregistered NIC and impact of data layout and 
use of no update in place structures on transmission throughput. The cluster has 7
Mellanox SX6036G FDR switches arranged in two layers. The switching fabric is
oversubscribed and provides about 16~Gbps of bisection bandwidth per node
when congested.

We started out by getting measurements of transmission speeds using RDMA Reads.
We implemented rdma operations using infinibands ib verbs library. We implemented 
RDMA reads and write as op codes on the \cpp{ibv_send_wr} request which 
transmit data as \cpp{ibv_post_send} and also tried to measure the transmit
performance while using zero copy using \cpp{IBV_WR_SEND} as the operation.
We found that send operations can transmit multiple transmit buffer
It became evident quickly that one sided RDMA reads are not well positioned to 
take advantage of the zero copy paradigm since it only supports a remote read 
from a given location from the source. We also tried implementing RDMA reads 