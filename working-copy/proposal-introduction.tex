%%% -*-LaTeX-*-

\chapter{Introduction}

For decades, the performance characteristics of storage devices have dominated
 thinking in database design and implementation. From data layout to
concurrency control, to recovery, to threading model, disks touch every aspect
of database design. In-memory databases effectively eliminate disk I/O as a
concern, and these systems now execute millions of operations per second,
sometimes with near-microsecond latencies.  It is tempting to believe 
database I/O is solved; however, another device now dictates
performance: the network interface (NIC). As the primary I/O device in present-day databases,
NIC should be a first class citizen in all phases of database design.
However, modern network cards have grown incredibly complex, partly in response
to demands for high throughput and low latency.

We set out to improve the efficiency of RAMCloud's~\cite{ramcloud} data migration mechanism. 
RAMCloud is a storage system that keeps all data in DRAM at all times and provides low latency access by
leveraging advancements like kernel bypass which are present in a modern NIC.
To thoroughly understand the complexities involved in NIC in doing huge
transfers of data, we profiled a modern NIC with a focus on heavy data transfers.
 We made several discoveries that informed our design for RAMCloud,
 but they also generalize to other transfer-heavy database operations. 

This thesis is structured into three key pieces. The first microbenchmarks a modern NIC 
with kernel bypass and RDMA and uses the findings to inform an efficient   migration design for RAMCloud.
The second piece is a breakdown of RAMCloud’s existing migration mechanism to understand its bottlenecks
and to assess why it is incapable of fully exploiting modern server hardware. 
Finally, our forward-looking objective is a new design for migration that we believe will advance state of the art
in terms of time taken to migrate large amounts of data within a cluster of in-memory storage systems
with minimal disruption to the normal case operation.

While developing the microbenchmark to analyze a NIC, one thing was becoming increasingly clearer to us.
Modern network cards have grown incredibly complex, partly in response to
demands for high throughput and low latency. One key advancement these NICs made 
is the presence of some form of kernel bypass where an on-device DMA engine from the network card 
can access application memory without invoking the CPU. This development led to the Remote 
Direct Memory Access (RDMA)~\cite{rdmapatent} paradigm. The modern NIC with its complex architecture
makes understanding its characteristics difficult, and it makes designing software to take advantage
of them even harder. Database designers encounter many trade-offs when trying to use the NIC effectively.

Our mission was to come up with a migration protocol that advances state of the art for in-memory storage systems.
We realized the importance of the making data transfer NIC friendly and set out to measure what variables impact the 
transmission performance. We wanted to enumerate and underline the challenges in designing keeping NIC as a first class citizen.
We specifically looked at three factors that make it especially challenging to design NIC-friendly
database engines:
\begin{itemize}
  \item {\em Zero-copy} DMA engines reduce server load for transferring large data
    blocks, but large, static blocks are uncommon for in-memory databases,
    where records can be small, and update rates can be high.
  \item The performance gains from zero-copy DMA do not generalize to
      {\em finer-grained objects} which are commonplace in in-memory stores for two reasons:
      \begin{enumerate}
        \item Transmit descriptors grow linearly in the number of records;
        \item NIC hardware limits descriptor length, restricting speed for small records.
      \end{enumerate}
      Despite this, we find that zero-copy can make sense for small objects under certain conditions, such as
      small ``add-ons'' to larger transmissions.
   \item Zero-copy introduces complications for \emph{locking and object
      lifetime}, as objects must remain in memory and must be
      unchanged for the life of the DMA operation (which includes transmission
      time).
  \item {\em Direct Cache Access}, now rephrased as Intel\textregistered Data Direct I/O~\cite{ddio},
      provide direct access to last level cache from an I/O device. This effect
      impacts the memory pressure significantly.

\end{itemize}


These factors make advanced NIC DMA difficult to use effectively. We set out to explore data 
transmission purely from the perspective of a modern NIC. We profiled a cluster of machines with
Infiniband ConnectX\textregistered-3, a modern kernel-bypass capable NIC
with RDMA capabilities, with specific attention to large transfers and query responses.
We published our findings and made some concrete suggestions around how NICs might
influence data layout and concurrency control in a modern in-memory 
database~\cite{KesavanRicciStutsman:IMDM16}. 

Our primary results gave us ideas about how the Zero Copy paradigm will perform against
the traditional approach of transmitting via larger buffers. These also pointed 
to how the use of thread local storage and effective network aware caching such as DDIO~\cite{ddio}
somehow refutes the argument for paging records together.

Another burden of using these newer storage systems are how they would provide effective 
reconfiguration. Our results from the benchmark led us to believe in the importance of 
hardware considerations in data migration. State of the art systems like RAMCloud has around 1000x
lower latency than traditional storage systems, and these tight SLAs make it harder to do migrations
without interfering with regular requests. This is also one of the key hurdles 
to overcome before mainstream acceptance of these systems.
Our findings about modern NICS inform a new design for cluster reconfiguration
in the RAMCloud storage system. Low-cost load balancing improves RAMCloud’s
efficiency since indexes~\cite{slik} and transactions~\cite{ramcloudtx} benefit from collocation, but, at the same time, RAMCloud’s exceedingly tight SLAs (5 $\mu$s median
access times and 99.9th of reads within 10$\mu$s) mean any disruption to performance
has a serious impact. For an in-memory database, we would ideally like to saturate the
available NIC bandwidth which is 8-12GB/s~\cite{cx3,cx4} while state of the art 
transfer mechanisms offer many orders of magnitudes lower than that at ~10MB/s 
throughput and only offer 1000-10000x higher latency than RAMCloud~\cite{ramcloud}.
We wanted our protocol to strike a balance between fast operation and minimal disruption without drastically affecting
the latency and throughput available for the system. Our design for fast and efficient migration
for RAMCloud is guided by the following key principles:
\begin{itemize}
\item{\textbf{High Throughput}}: State of the art systems do migrations at $<$100 Mb/s. Our plan
should result in 100-1000x faster data migrations
\item{\textbf{Low Impact}}: RAMCloud has 99.9 percentile read latency at $<$10$\mu$s and write latency
at $<$100$\mu$s. Our proposed protocol will be of minimal impact to the normal case operation, still keeping
up with SLAs that are 10-100x tighter than other systems~\cite{squall}.
\item{\textbf{Late-partitioning}}: RAMCloud promises uniform access latency regardless of the level of data locality.
Most other systems diverge from this approach to make data movement faster. We would like to stick with
deferring partitioning decisions as much as possible and this adds a challenge in preparing data for migration.
\end{itemize}


We evaluate the current state of the art approaches to migration in RAMCloud storage system.
We lay out some experiments that highlight issues with current approaches and motivates our new 
fast and efficient migration protocol. To motivate and demonstrate our approach, we explore
the following experiments:
\begin{itemize}
\item Measuring impacts of throughput at {\em source of migration}; Load balancing is a key problem that 
could be addressed with migration and measuring the overhead introduced by migration is helpful here.
\item Analyzing the effect of splitting and migrating {\em hot indexes}; Indexed reads add value to in-memory
stores in utilizing them as a complete database. 
\item Exploring effects of {\em locality of data}; RAMCloud's unique take on locality makes its distributed 
recovery~\cite{ongaro2011fast,ryan-thesis} interesting because of its effect of distributing recovered data.
We will explore the impact of recovery on locality and how reconfiguration can repair to restore locality.
\item {\em Scaling clusters up and down} will show the effects of data movement in many-to-one and one-to-many
data redistributions.
\end{itemize} 
 These will explore the themes of data locality,
source impact and the effect of numerous workloads which include scaling up and down the cluster, splitting
and migrating hot index tables, the impact on locality by the distributed recovery which sprays data across the 
cluster and responding to changes in access patterns taking into account the multi-dimensional resource contentions
that could occur in a practical system.

We will also propose the outline for an effective migration protocol that leverages
the log structured memory of RAMCloud as well as the distributed crash recovery mechanism
to provide fast and jitter-free data movement in a cluster for reconfiguration and improved 
performance for range queries and distributed transactions. We will try to do so while
deferring partitioning decisions as much as possible.

\section{Contributions}

\textbf{\blockquote{\enquote{Scale-out in-memory stores are optimized for small requests
under tight SLAs, and bulk data movement, for rebalancing and range queries, interfere;
We hypothesize that carefully leveraging data layout and advancements in modern NICs
will yield to gains in performance and efficiency for large transfers in these systems
without disrupting their primary obligations}}}
 
We have shown that careful co-design of data layout, concurrency control
and networking can lead up 75\% reduction of CPU overhead and  get a throughput
boost of 1.7x compared to blindly using the kernel bypass available
in the NICs. We also profiled the bottlenecks in the way data migration is set up
in RAMCloud and in conjunction with the benchmarks. Though there were efforts to 
dismantle cost of a database in the network layer before, the server-side impact
on mixed workloads including range scans and migrations have not been well understood
until now. In the light of our discoveries, we make concrete suggestions on how
 to design an fast and efficient migration 
protocol which causes minimal disruption.

The following are the main contributions from this work which supports and provides
evidence for the thesis statement.

\begin{itemize}

\item{\textbf{Evaluation of Impact of Modern NICs on data layout}}: This is the first published research exploring the scatter gather DMA engine of the NIC in contrast with the traditional copy out approach in the context of bulk transfers. 
 We profiled the transmit performance of a modern NIC in depth to show the influence of NICs on data layout. 
 We have found a few arguments against the blind use of the new kernel bypass approach considering
 the overhead of concurrency management and difficulty of implementation.
\begin{itemize}

\item Developed a microbenchmark to assess performance of infiniband verbs

\item Measured transmission throughput and CPU overhead in the face of varying
data layouts

\item Compared the performance of Zero-Copy and Copy Out mechanisms of 
transmitting data from an RDMA capable modern NIC.

\end{itemize} 


\item{\textbf{Experiments to motivate fast and efficient data transfer}}: This is the first assessment of the impact of data migration on SLAs for a high performance,
  in-memory store that leverages high throughput, kernel-bypass networking fabrics. 
We laid out experiments that show how even the state of the art systems for cluster reconfiguration becomes suboptimal with its locality constraints and overall slowness. 
 RAMCloud assumes relatively uniform access cost to all nodes.
 We quantify the gains from explicitly colocating records and indexes.
 Our experiments call for fast and efficient migration protocol especially suited for an In-memory database system with a decentralized log structured data model such as RAMCloud.

\item{\textbf{The design of client assisted responses for better transmit performance}}: Our measurements inform a new design for migration in RAMCloud, but it also has impacts on other database operations.
 While evaluating the results, we saw that there are certain structures and circumstances that can uniquely take advantage of the hardware limited scatter gather entries of a modern NIC.
 Chapter~\ref{chap:bw-tree} explores ways in which a modern B-tree indices can exploit NIC hardware.
 Through the benchmark, we show 70\% improvement in throughput using such a design.


\item{\textbf{Preliminary design for a migration protocol}}: After careful analysis of the current migration protocol
 we discovered the most important bottlenecks in the current design that makes it infeasible for fast and efficient migration.
 We propose the basics of a migration protocol that will effectively eliminate these bottlenecks and pave the way
 forward for fast and efficient transfer of huge chunks of data.
\end{itemize}

