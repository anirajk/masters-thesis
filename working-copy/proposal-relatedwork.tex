%%% -*-LaTeX-*-

\chapter{Related Work}
In-memory databases and low latency networks are very heavily researched areas
these days. Our work borrows concepts from numerous efforts both $\grave{a}$-
la-mode and prior from the databases, networking and systems research communities.
RAMCloud~\cite{ramcloud} itself was a research product built on top of other 
work on log-structured file systems, striping filesystems and distributed 
crash recovery~\cite{ryan-thesis}

\section{In-memory Databases}
There have been many systems that were using DRAM as a caching mechanism even before
In-memory database systems became commonplace. Memcached~\cite{memcached-orig} is 
the most popular form of such generic caching mechanism. Declining cost of DRAM 
technologies paved way for designs that are completely reliant on DRAM. Main memory
database systems of today offers a plethora of features that are at par with traditional
database systems including data storage and indexing, concurrency control, durability and 
recovery techniques, query processing and compilation, support for high availability, and 
ability to support hybrid transactional and analytics workloads~\cite{mmdbmstutorial}
FaRM~\cite{farm} is a phenomenal In-memory, distributed computing platform that leverage lock free reads over RDMA~\cite{rdma},
collocating objects and function shipping to provide performance characteristics of the 
highest order for an In-memory database. They could even scale distributed transactions to
hundreds of millions of operations with recovery times of the order of milliseconds. There 
are other systems such as Silo~\cite{silo} which effectively utilises multi core CPUs.

\subsection{Lock freedom}
Lock freedom is a design philosophy which was perfected on previous work on fine grained concurrency 
primitives~\cite{finegrained} and lock free hash tables ~\cite{lockfreeht}. Lock freedom is commonly
used in databases these days~\cite{htm} since they gained practicality by common use of
epochs~\cite{lockfreedom} based reclamation. 
More recent iterations of SQL engines such as Hekaton~\cite{hekaton} are 
examples for In-memory optimised  database engines. Bw-Tree~\cite{bw-tree}, a 
data structure that is lock and latch free and follows a no-update-in-place philosophy to indexing.
Others have worked on exploiting Hardware Transactional Memory~\cite{htm-old}
for In-memory transaction processing~\cite{drtm} as well as lock free indexing~\cite{htm}.

\section{RDMA in Databases}
The merits of RDMA have been widely accepted by database researchers by now. From
the early arguments~\cite{rdmacase} calling for the need for offloading CPUs to 
research aiming for a billion key value operations on key value servers ~\cite{rdmabillion},
RDMA has proven to be the future of data transfer for large scale databases. Key value stores
such as HERD~\cite{herd} and MICA~\cite{mica} and systems that provide more complex
data models such as RAMCloud~\cite{ramcloud} use RDMA effectively and there have been 
published guidelines to maximise performance for dispatch heavy workloads~\cite{rdma}

\section{Data Transfer in Fast Networks}
As pointed out by various researchers in the past, the days of slow networks are over~\cite{slow}
and the thesis argues heavily in favor of that fact. Several efforts in the past have exploited
user level NIC access as well as RDMA for highly performant distributed database systems~\cite{ramcloud,farm,farmtx,drtm,hyper}
Our findings should also complement other efforts such as transactional key value stores~\cite{deuteronomy} 
and shared-data databases~\cite{tell}. Small, fixed-size request/response cycles have been optimized in
existing research~\cite{farm,herd,mica,rdma,ramcloud}, but the efficient
transmission of larger responses like range query results or data
migrations has been less well studied. Studies focused on large transmissions
have so far been limited to relatively static block-oriented data.
Our work focuses on optimizing transmission of large and complex query
results, which differs from these two categories.

A complementary study by Kalia, Kaminsky, and Andersen~\cite{rdma} provides a
different analysis of host interaction with Mellanox Infiniband network adapters,
and they extract rules of thumb to help developers get the best performance from the hardware.
The low-level nature of their analysis is especially suited for
optimizing dispatch-heavy workloads with a high-volume of small
request-response operations. Our focus is on operations that require heavy
responses where the precise size or the content of the response cannot be anticipated or is
under heavy mutation.